{"id":"09549143-f9c3-43e2-b1f5-47fd8b1ea0a5","name":"RecycleBot","data":{"nodes":[{"id":"ChatInput-feB0F","type":"genericNode","position":{"x":381.01351442793407,"y":1268.515136617918},"data":{"node":{"template":{"_type":"Component","files":{"trace_as_metadata":true,"file_path":"","fileTypes":["txt","md","mdx","csv","json","yaml","yml","xml","html","htm","pdf","docx","py","sh","sql","js","ts","tsx","jpg","jpeg","png","bmp","image"],"list":true,"list_add_label":"Add More","required":false,"placeholder":"","show":true,"name":"files","value":"","display_name":"Files","advanced":true,"dynamic":false,"info":"Files to be sent with the message.","title_case":false,"type":"file","_input_type":"FileInput"},"background_color":{"tool_mode":false,"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"list_add_label":"Add More","required":false,"placeholder":"","show":true,"name":"background_color","value":"","display_name":"Background Color","advanced":true,"input_types":["Message"],"dynamic":false,"info":"The background color of the icon.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"chat_icon":{"tool_mode":false,"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"list_add_label":"Add More","required":false,"placeholder":"","show":true,"name":"chat_icon","value":"","display_name":"Icon","advanced":true,"input_types":["Message"],"dynamic":false,"info":"The icon of the message.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.data.utils import IMG_FILE_TYPES, TEXT_FILE_TYPES\nfrom langflow.base.io.chat import ChatComponent\nfrom langflow.inputs import BoolInput\nfrom langflow.io import (\n    DropdownInput,\n    FileInput,\n    MessageTextInput,\n    MultilineInput,\n    Output,\n)\nfrom langflow.schema.message import Message\nfrom langflow.utils.constants import (\n    MESSAGE_SENDER_AI,\n    MESSAGE_SENDER_NAME_USER,\n    MESSAGE_SENDER_USER,\n)\n\n\nclass ChatInput(ChatComponent):\n    display_name = \"Chat Input\"\n    description = \"Get chat inputs from the Playground.\"\n    icon = \"MessagesSquare\"\n    name = \"ChatInput\"\n    minimized = True\n\n    inputs = [\n        MultilineInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            value=\"\",\n            info=\"Message to be passed as input.\",\n            input_types=[],\n        ),\n        BoolInput(\n            name=\"should_store_message\",\n            display_name=\"Store Messages\",\n            info=\"Store the message in the history.\",\n            value=True,\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"sender\",\n            display_name=\"Sender Type\",\n            options=[MESSAGE_SENDER_AI, MESSAGE_SENDER_USER],\n            value=MESSAGE_SENDER_USER,\n            info=\"Type of sender.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"sender_name\",\n            display_name=\"Sender Name\",\n            info=\"Name of the sender.\",\n            value=MESSAGE_SENDER_NAME_USER,\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"session_id\",\n            display_name=\"Session ID\",\n            info=\"The session ID of the chat. If empty, the current session ID parameter will be used.\",\n            advanced=True,\n        ),\n        FileInput(\n            name=\"files\",\n            display_name=\"Files\",\n            file_types=TEXT_FILE_TYPES + IMG_FILE_TYPES,\n            info=\"Files to be sent with the message.\",\n            advanced=True,\n            is_list=True,\n        ),\n        MessageTextInput(\n            name=\"background_color\",\n            display_name=\"Background Color\",\n            info=\"The background color of the icon.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"chat_icon\",\n            display_name=\"Icon\",\n            info=\"The icon of the message.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"text_color\",\n            display_name=\"Text Color\",\n            info=\"The text color of the name\",\n            advanced=True,\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Message\", name=\"message\", method=\"message_response\"),\n    ]\n\n    async def message_response(self) -> Message:\n        background_color = self.background_color\n        text_color = self.text_color\n        icon = self.chat_icon\n\n        message = await Message.create(\n            text=self.input_value,\n            sender=self.sender,\n            sender_name=self.sender_name,\n            session_id=self.session_id,\n            files=self.files,\n            properties={\n                \"background_color\": background_color,\n                \"text_color\": text_color,\n                \"icon\": icon,\n            },\n        )\n        if self.session_id and isinstance(message, Message) and self.should_store_message:\n            stored_message = await self.send_message(\n                message,\n            )\n            self.message.value = stored_message\n            message = stored_message\n\n        self.status = message\n        return message\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"input_value":{"tool_mode":false,"trace_as_input":true,"multiline":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"list_add_label":"Add More","required":false,"placeholder":"","show":true,"name":"input_value","value":"","display_name":"Text","advanced":false,"input_types":[],"dynamic":false,"info":"Message to be passed as input.","title_case":false,"type":"str","_input_type":"MultilineInput"},"sender":{"tool_mode":false,"trace_as_metadata":true,"options":["Machine","User"],"options_metadata":[],"combobox":false,"dialog_inputs":{},"required":false,"placeholder":"","show":true,"name":"sender","value":"User","display_name":"Sender Type","advanced":true,"dynamic":false,"info":"Type of sender.","title_case":false,"type":"str","_input_type":"DropdownInput"},"sender_name":{"tool_mode":false,"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"list_add_label":"Add More","required":false,"placeholder":"","show":true,"name":"sender_name","value":"User","display_name":"Sender Name","advanced":true,"input_types":["Message"],"dynamic":false,"info":"Name of the sender.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"session_id":{"tool_mode":false,"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"list_add_label":"Add More","required":false,"placeholder":"","show":true,"name":"session_id","value":"","display_name":"Session ID","advanced":true,"input_types":["Message"],"dynamic":false,"info":"The session ID of the chat. If empty, the current session ID parameter will be used.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"should_store_message":{"tool_mode":false,"trace_as_metadata":true,"list":false,"list_add_label":"Add More","required":false,"placeholder":"","show":true,"name":"should_store_message","value":true,"display_name":"Store Messages","advanced":true,"dynamic":false,"info":"Store the message in the history.","title_case":false,"type":"bool","_input_type":"BoolInput"},"text_color":{"tool_mode":false,"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"list_add_label":"Add More","required":false,"placeholder":"","show":true,"name":"text_color","value":"","display_name":"Text Color","advanced":true,"input_types":["Message"],"dynamic":false,"info":"The text color of the name","title_case":false,"type":"str","_input_type":"MessageTextInput"}},"description":"Get chat inputs from the Playground.","icon":"MessagesSquare","base_classes":["Message"],"display_name":"Chat Input","documentation":"","minimized":true,"custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"message","display_name":"Message","method":"message_response","value":"__UNDEFINED__","cache":true,"allows_loop":false,"tool_mode":true}],"field_order":["input_value","should_store_message","sender","sender_name","session_id","files","background_color","chat_icon","text_color"],"beta":false,"legacy":false,"edited":false,"metadata":{},"tool_mode":false,"lf_version":"1.2.0"},"showNode":false,"type":"ChatInput","id":"ChatInput-feB0F"},"selected":false,"measured":{"width":192,"height":66}},{"id":"ChatOutput-vikhM","type":"genericNode","position":{"x":1987.9316723132026,"y":683.0396843229653},"data":{"node":{"template":{"_type":"Component","input_value":{"trace_as_metadata":true,"list":false,"list_add_label":"Add More","required":true,"placeholder":"","show":true,"name":"input_value","value":"","display_name":"Text","advanced":false,"input_types":["Data","DataFrame","Message"],"dynamic":false,"info":"Message to be passed as output.","title_case":false,"type":"other","_input_type":"HandleInput"},"background_color":{"tool_mode":false,"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"list_add_label":"Add More","required":false,"placeholder":"","show":true,"name":"background_color","value":"","display_name":"Background Color","advanced":true,"input_types":["Message"],"dynamic":false,"info":"The background color of the icon.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"chat_icon":{"tool_mode":false,"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"list_add_label":"Add More","required":false,"placeholder":"","show":true,"name":"chat_icon","value":"","display_name":"Icon","advanced":true,"input_types":["Message"],"dynamic":false,"info":"The icon of the message.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"clean_data":{"tool_mode":false,"trace_as_metadata":true,"list":false,"list_add_label":"Add More","required":false,"placeholder":"","show":true,"name":"clean_data","value":true,"display_name":"Basic Clean Data","advanced":true,"dynamic":false,"info":"Whether to clean the data","title_case":false,"type":"bool","_input_type":"BoolInput"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from collections.abc import Generator\nfrom typing import Any\n\nfrom langflow.base.io.chat import ChatComponent\nfrom langflow.inputs import BoolInput\nfrom langflow.inputs.inputs import HandleInput\nfrom langflow.io import DropdownInput, MessageTextInput, Output\nfrom langflow.schema.data import Data\nfrom langflow.schema.dataframe import DataFrame\nfrom langflow.schema.message import Message\nfrom langflow.schema.properties import Source\nfrom langflow.utils.constants import (\n    MESSAGE_SENDER_AI,\n    MESSAGE_SENDER_NAME_AI,\n    MESSAGE_SENDER_USER,\n)\n\n\nclass ChatOutput(ChatComponent):\n    display_name = \"Chat Output\"\n    description = \"Display a chat message in the Playground.\"\n    icon = \"MessagesSquare\"\n    name = \"ChatOutput\"\n    minimized = True\n\n    inputs = [\n        HandleInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            info=\"Message to be passed as output.\",\n            input_types=[\"Data\", \"DataFrame\", \"Message\"],\n            required=True,\n        ),\n        BoolInput(\n            name=\"should_store_message\",\n            display_name=\"Store Messages\",\n            info=\"Store the message in the history.\",\n            value=True,\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"sender\",\n            display_name=\"Sender Type\",\n            options=[MESSAGE_SENDER_AI, MESSAGE_SENDER_USER],\n            value=MESSAGE_SENDER_AI,\n            advanced=True,\n            info=\"Type of sender.\",\n        ),\n        MessageTextInput(\n            name=\"sender_name\",\n            display_name=\"Sender Name\",\n            info=\"Name of the sender.\",\n            value=MESSAGE_SENDER_NAME_AI,\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"session_id\",\n            display_name=\"Session ID\",\n            info=\"The session ID of the chat. If empty, the current session ID parameter will be used.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"data_template\",\n            display_name=\"Data Template\",\n            value=\"{text}\",\n            advanced=True,\n            info=\"Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.\",\n        ),\n        MessageTextInput(\n            name=\"background_color\",\n            display_name=\"Background Color\",\n            info=\"The background color of the icon.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"chat_icon\",\n            display_name=\"Icon\",\n            info=\"The icon of the message.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"text_color\",\n            display_name=\"Text Color\",\n            info=\"The text color of the name\",\n            advanced=True,\n        ),\n        BoolInput(\n            name=\"clean_data\",\n            display_name=\"Basic Clean Data\",\n            value=True,\n            info=\"Whether to clean the data\",\n            advanced=True,\n        ),\n    ]\n    outputs = [\n        Output(\n            display_name=\"Message\",\n            name=\"message\",\n            method=\"message_response\",\n        ),\n    ]\n\n    def _build_source(self, id_: str | None, display_name: str | None, source: str | None) -> Source:\n        source_dict = {}\n        if id_:\n            source_dict[\"id\"] = id_\n        if display_name:\n            source_dict[\"display_name\"] = display_name\n        if source:\n            # Handle case where source is a ChatOpenAI object\n            if hasattr(source, \"model_name\"):\n                source_dict[\"source\"] = source.model_name\n            elif hasattr(source, \"model\"):\n                source_dict[\"source\"] = str(source.model)\n            else:\n                source_dict[\"source\"] = str(source)\n        return Source(**source_dict)\n\n    async def message_response(self) -> Message:\n        # First convert the input to string if needed\n        text = self.convert_to_string()\n        # Get source properties\n        source, icon, display_name, source_id = self.get_properties_from_source_component()\n        background_color = self.background_color\n        text_color = self.text_color\n        if self.chat_icon:\n            icon = self.chat_icon\n\n        # Create or use existing Message object\n        if isinstance(self.input_value, Message):\n            message = self.input_value\n            # Update message properties\n            message.text = text\n        else:\n            message = Message(text=text)\n\n        # Set message properties\n        message.sender = self.sender\n        message.sender_name = self.sender_name\n        message.session_id = self.session_id\n        message.flow_id = self.graph.flow_id if hasattr(self, \"graph\") else None\n        message.properties.source = self._build_source(source_id, display_name, source)\n        message.properties.icon = icon\n        message.properties.background_color = background_color\n        message.properties.text_color = text_color\n\n        # Store message if needed\n        if self.session_id and self.should_store_message:\n            stored_message = await self.send_message(message)\n            self.message.value = stored_message\n            message = stored_message\n\n        self.status = message\n        return message\n\n    def _validate_input(self) -> None:\n        \"\"\"Validate the input data and raise ValueError if invalid.\"\"\"\n        if self.input_value is None:\n            msg = \"Input data cannot be None\"\n            raise ValueError(msg)\n        if isinstance(self.input_value, list) and not all(\n            isinstance(item, Message | Data | DataFrame | str) for item in self.input_value\n        ):\n            invalid_types = [\n                type(item).__name__\n                for item in self.input_value\n                if not isinstance(item, Message | Data | DataFrame | str)\n            ]\n            msg = f\"Expected Data or DataFrame or Message or str, got {invalid_types}\"\n            raise TypeError(msg)\n        if not isinstance(\n            self.input_value,\n            Message | Data | DataFrame | str | list | Generator | type(None),\n        ):\n            type_name = type(self.input_value).__name__\n            msg = f\"Expected Data or DataFrame or Message or str, Generator or None, got {type_name}\"\n            raise TypeError(msg)\n\n    def _safe_convert(self, data: Any) -> str:\n        \"\"\"Safely convert input data to string.\"\"\"\n        try:\n            if isinstance(data, str):\n                return data\n            if isinstance(data, Message):\n                return data.get_text()\n            if isinstance(data, Data):\n                if data.get_text() is None:\n                    msg = \"Empty Data object\"\n                    raise ValueError(msg)\n                return data.get_text()\n            if isinstance(data, DataFrame):\n                if self.clean_data:\n                    # Remove empty rows\n                    data = data.dropna(how=\"all\")\n                    # Remove empty lines in each cell\n                    data = data.replace(r\"^\\s*$\", \"\", regex=True)\n                    # Replace multiple newlines with a single newline\n                    data = data.replace(r\"\\n+\", \"\\n\", regex=True)\n                return (\n                    data.replace(r\"\\|\", r\"\\\\|\", regex=True)\n                    .applymap(lambda x: (str(x).replace(\"\\n\", \"<br/>\") if isinstance(x, str) else x))\n                    .to_markdown(index=False)\n                )\n            return str(data)\n        except (ValueError, TypeError, AttributeError) as e:\n            msg = f\"Error converting data: {e!s}\"\n            raise ValueError(msg) from e\n\n    def convert_to_string(self) -> str | Generator[Any, None, None]:\n        \"\"\"Convert input data to string with proper error handling.\"\"\"\n        self._validate_input()\n        if isinstance(self.input_value, list):\n            return \"\\n\".join([self._safe_convert(item) for item in self.input_value])\n        if isinstance(self.input_value, Generator):\n            return self.input_value\n        return self._safe_convert(self.input_value)\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"data_template":{"tool_mode":false,"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"list_add_label":"Add More","required":false,"placeholder":"","show":true,"name":"data_template","value":"{text}","display_name":"Data Template","advanced":true,"input_types":["Message"],"dynamic":false,"info":"Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"sender":{"tool_mode":false,"trace_as_metadata":true,"options":["Machine","User"],"options_metadata":[],"combobox":false,"dialog_inputs":{},"required":false,"placeholder":"","show":true,"name":"sender","value":"Machine","display_name":"Sender Type","advanced":true,"dynamic":false,"info":"Type of sender.","title_case":false,"type":"str","_input_type":"DropdownInput"},"sender_name":{"tool_mode":false,"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"list_add_label":"Add More","required":false,"placeholder":"","show":true,"name":"sender_name","value":"AI","display_name":"Sender Name","advanced":true,"input_types":["Message"],"dynamic":false,"info":"Name of the sender.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"session_id":{"tool_mode":false,"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"list_add_label":"Add More","required":false,"placeholder":"","show":true,"name":"session_id","value":"","display_name":"Session ID","advanced":true,"input_types":["Message"],"dynamic":false,"info":"The session ID of the chat. If empty, the current session ID parameter will be used.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"should_store_message":{"tool_mode":false,"trace_as_metadata":true,"list":false,"list_add_label":"Add More","required":false,"placeholder":"","show":true,"name":"should_store_message","value":true,"display_name":"Store Messages","advanced":true,"dynamic":false,"info":"Store the message in the history.","title_case":false,"type":"bool","_input_type":"BoolInput"},"text_color":{"tool_mode":false,"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"list_add_label":"Add More","required":false,"placeholder":"","show":true,"name":"text_color","value":"","display_name":"Text Color","advanced":true,"input_types":["Message"],"dynamic":false,"info":"The text color of the name","title_case":false,"type":"str","_input_type":"MessageTextInput"}},"description":"Display a chat message in the Playground.","icon":"MessagesSquare","base_classes":["Message"],"display_name":"Chat Output","documentation":"","minimized":true,"custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"message","display_name":"Message","method":"message_response","value":"__UNDEFINED__","cache":true,"allows_loop":false,"tool_mode":true}],"field_order":["input_value","should_store_message","sender","sender_name","session_id","data_template","background_color","chat_icon","text_color","clean_data"],"beta":false,"legacy":false,"edited":false,"metadata":{},"tool_mode":false,"lf_version":"1.2.0"},"showNode":false,"type":"ChatOutput","id":"ChatOutput-vikhM"},"selected":false,"measured":{"width":192,"height":66}},{"id":"GoogleGenerativeAIModel-imboU","type":"genericNode","position":{"x":1312.9474561649388,"y":390},"data":{"node":{"template":{"_type":"Component","api_key":{"load_from_db":true,"required":true,"placeholder":"","show":true,"name":"api_key","value":"","display_name":"Google API Key","advanced":false,"input_types":["Message"],"dynamic":false,"info":"The Google API Key to use for the Google Generative AI.","real_time_refresh":true,"title_case":false,"password":true,"type":"str","_input_type":"SecretStrInput"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from typing import Any\n\nimport requests\nfrom loguru import logger\nfrom pydantic.v1 import SecretStr\n\nfrom langflow.base.models.google_generative_ai_constants import GOOGLE_GENERATIVE_AI_MODELS\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import LanguageModel\nfrom langflow.field_typing.range_spec import RangeSpec\nfrom langflow.inputs import DropdownInput, FloatInput, IntInput, SecretStrInput, SliderInput\nfrom langflow.inputs.inputs import BoolInput\nfrom langflow.schema import dotdict\n\n\nclass GoogleGenerativeAIComponent(LCModelComponent):\n    display_name = \"Google Generative AI\"\n    description = \"Generate text using Google Generative AI.\"\n    icon = \"GoogleGenerativeAI\"\n    name = \"GoogleGenerativeAIModel\"\n\n    inputs = [\n        *LCModelComponent._base_inputs,\n        IntInput(\n            name=\"max_output_tokens\", display_name=\"Max Output Tokens\", info=\"The maximum number of tokens to generate.\"\n        ),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model\",\n            info=\"The name of the model to use.\",\n            options=GOOGLE_GENERATIVE_AI_MODELS,\n            value=\"gemini-1.5-pro\",\n            refresh_button=True,\n            combobox=True,\n        ),\n        SecretStrInput(\n            name=\"api_key\",\n            display_name=\"Google API Key\",\n            info=\"The Google API Key to use for the Google Generative AI.\",\n            required=True,\n            real_time_refresh=True,\n        ),\n        FloatInput(\n            name=\"top_p\",\n            display_name=\"Top P\",\n            info=\"The maximum cumulative probability of tokens to consider when sampling.\",\n            advanced=True,\n        ),\n        SliderInput(\n            name=\"temperature\",\n            display_name=\"Temperature\",\n            value=0.1,\n            range_spec=RangeSpec(min=0, max=2, step=0.01),\n            info=\"Controls randomness. Lower values are more deterministic, higher values are more creative.\",\n        ),\n        IntInput(\n            name=\"n\",\n            display_name=\"N\",\n            info=\"Number of chat completions to generate for each prompt. \"\n            \"Note that the API may not return the full n completions if duplicates are generated.\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"top_k\",\n            display_name=\"Top K\",\n            info=\"Decode using top-k sampling: consider the set of top_k most probable tokens. Must be positive.\",\n            advanced=True,\n        ),\n        BoolInput(\n            name=\"tool_model_enabled\",\n            display_name=\"Tool Model Enabled\",\n            info=\"Whether to use the tool model.\",\n            value=False,\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        try:\n            from langchain_google_genai import ChatGoogleGenerativeAI\n        except ImportError as e:\n            msg = \"The 'langchain_google_genai' package is required to use the Google Generative AI model.\"\n            raise ImportError(msg) from e\n\n        google_api_key = self.api_key\n        model = self.model_name\n        max_output_tokens = self.max_output_tokens\n        temperature = self.temperature\n        top_k = self.top_k\n        top_p = self.top_p\n        n = self.n\n\n        return ChatGoogleGenerativeAI(\n            model=model,\n            max_output_tokens=max_output_tokens or None,\n            temperature=temperature,\n            top_k=top_k or None,\n            top_p=top_p or None,\n            n=n or 1,\n            google_api_key=SecretStr(google_api_key).get_secret_value(),\n        )\n\n    def get_models(self, tool_model_enabled: bool | None = None) -> list[str]:\n        try:\n            import google.generativeai as genai\n\n            genai.configure(api_key=self.api_key)\n            model_ids = [\n                model.name.replace(\"models/\", \"\")\n                for model in genai.list_models()\n                if \"generateContent\" in model.supported_generation_methods\n            ]\n            model_ids.sort(reverse=True)\n        except (ImportError, ValueError) as e:\n            logger.exception(f\"Error getting model names: {e}\")\n            model_ids = GOOGLE_GENERATIVE_AI_MODELS\n        if tool_model_enabled:\n            try:\n                from langchain_google_genai.chat_models import ChatGoogleGenerativeAI\n            except ImportError as e:\n                msg = \"langchain_google_genai is not installed.\"\n                raise ImportError(msg) from e\n            for model in model_ids:\n                model_with_tool = ChatGoogleGenerativeAI(\n                    model=self.model_name,\n                    google_api_key=self.api_key,\n                )\n                if not self.supports_tool_calling(model_with_tool):\n                    model_ids.remove(model)\n        return model_ids\n\n    def update_build_config(self, build_config: dotdict, field_value: Any, field_name: str | None = None):\n        if field_name in (\"base_url\", \"model_name\", \"tool_model_enabled\", \"api_key\") and field_value:\n            try:\n                if len(self.api_key) == 0:\n                    ids = GOOGLE_GENERATIVE_AI_MODELS\n                else:\n                    try:\n                        ids = self.get_models(tool_model_enabled=self.tool_model_enabled)\n                    except (ImportError, ValueError, requests.exceptions.RequestException) as e:\n                        logger.exception(f\"Error getting model names: {e}\")\n                        ids = GOOGLE_GENERATIVE_AI_MODELS\n                build_config[\"model_name\"][\"options\"] = ids\n                build_config[\"model_name\"][\"value\"] = ids[0]\n            except Exception as e:\n                msg = f\"Error getting model names: {e}\"\n                raise ValueError(msg) from e\n        return build_config\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"input_value":{"trace_as_input":true,"tool_mode":false,"trace_as_metadata":true,"load_from_db":false,"list":false,"list_add_label":"Add More","required":false,"placeholder":"","show":true,"name":"input_value","value":"","display_name":"Input","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"MessageInput"},"max_output_tokens":{"tool_mode":false,"trace_as_metadata":true,"list":false,"list_add_label":"Add More","required":false,"placeholder":"","show":true,"name":"max_output_tokens","value":"","display_name":"Max Output Tokens","advanced":false,"dynamic":false,"info":"The maximum number of tokens to generate.","title_case":false,"type":"int","_input_type":"IntInput"},"model_name":{"tool_mode":false,"trace_as_metadata":true,"options":["learnlm-1.5-pro-experimental","gemma-3-27b-it","gemini-pro-vision","gemini-exp-1206","gemini-2.0-pro-exp-02-05","gemini-2.0-pro-exp","gemini-2.0-flash-thinking-exp-1219","gemini-2.0-flash-thinking-exp-01-21","gemini-2.0-flash-thinking-exp","gemini-2.0-flash-lite-preview-02-05","gemini-2.0-flash-lite-preview","gemini-2.0-flash-lite-001","gemini-2.0-flash-lite","gemini-2.0-flash-exp-image-generation","gemini-2.0-flash-exp","gemini-2.0-flash-001","gemini-2.0-flash","gemini-1.5-pro-latest","gemini-1.5-pro-002","gemini-1.5-pro-001","gemini-1.5-pro","gemini-1.5-flash-latest","gemini-1.5-flash-8b-latest","gemini-1.5-flash-8b-exp-0924","gemini-1.5-flash-8b-exp-0827","gemini-1.5-flash-8b-001","gemini-1.5-flash-8b","gemini-1.5-flash-002","gemini-1.5-flash-001-tuning","gemini-1.5-flash-001","gemini-1.5-flash","gemini-1.0-pro-vision-latest"],"options_metadata":[],"combobox":true,"dialog_inputs":{},"required":false,"placeholder":"","show":true,"name":"model_name","value":"learnlm-1.5-pro-experimental","display_name":"Model","advanced":false,"dynamic":false,"info":"The name of the model to use.","refresh_button":true,"title_case":false,"type":"str","_input_type":"DropdownInput"},"n":{"tool_mode":false,"trace_as_metadata":true,"list":false,"list_add_label":"Add More","required":false,"placeholder":"","show":true,"name":"n","value":"","display_name":"N","advanced":true,"dynamic":false,"info":"Number of chat completions to generate for each prompt. Note that the API may not return the full n completions if duplicates are generated.","title_case":false,"type":"int","_input_type":"IntInput"},"stream":{"tool_mode":false,"trace_as_metadata":true,"list":false,"list_add_label":"Add More","required":false,"placeholder":"","show":true,"name":"stream","value":false,"display_name":"Stream","advanced":false,"dynamic":false,"info":"Stream the response from the model. Streaming works only in Chat.","title_case":false,"type":"bool","_input_type":"BoolInput"},"system_message":{"tool_mode":false,"trace_as_input":true,"multiline":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"list_add_label":"Add More","required":false,"placeholder":"","show":true,"name":"system_message","value":"","display_name":"System Message","advanced":false,"input_types":["Message"],"dynamic":false,"info":"System message to pass to the model.","title_case":false,"type":"str","_input_type":"MultilineInput"},"temperature":{"tool_mode":false,"min_label":"","max_label":"","min_label_icon":"","max_label_icon":"","slider_buttons":false,"slider_buttons_options":[],"slider_input":false,"range_spec":{"step_type":"float","min":0,"max":2,"step":0.01},"required":false,"placeholder":"","show":true,"name":"temperature","value":0.1,"display_name":"Temperature","advanced":false,"dynamic":false,"info":"Controls randomness. Lower values are more deterministic, higher values are more creative.","title_case":false,"type":"slider","_input_type":"SliderInput"},"tool_model_enabled":{"tool_mode":false,"trace_as_metadata":true,"list":false,"list_add_label":"Add More","required":false,"placeholder":"","show":true,"name":"tool_model_enabled","value":false,"display_name":"Tool Model Enabled","advanced":false,"dynamic":false,"info":"Whether to use the tool model.","title_case":false,"type":"bool","_input_type":"BoolInput"},"top_k":{"tool_mode":false,"trace_as_metadata":true,"list":false,"list_add_label":"Add More","required":false,"placeholder":"","show":true,"name":"top_k","value":"","display_name":"Top K","advanced":true,"dynamic":false,"info":"Decode using top-k sampling: consider the set of top_k most probable tokens. Must be positive.","title_case":false,"type":"int","_input_type":"IntInput"},"top_p":{"tool_mode":false,"trace_as_metadata":true,"list":false,"list_add_label":"Add More","required":false,"placeholder":"","show":true,"name":"top_p","value":"","display_name":"Top P","advanced":true,"dynamic":false,"info":"The maximum cumulative probability of tokens to consider when sampling.","title_case":false,"type":"float","_input_type":"FloatInput"}},"description":"Generate text using Google Generative AI.","icon":"GoogleGenerativeAI","base_classes":["LanguageModel","Message"],"display_name":"Google Generative AI","documentation":"","minimized":false,"custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"text_output","hidden":null,"display_name":"Message","method":"text_response","value":"__UNDEFINED__","cache":true,"required_inputs":[],"allows_loop":false,"tool_mode":true},{"types":["LanguageModel"],"selected":"LanguageModel","name":"model_output","hidden":null,"display_name":"Language Model","method":"build_model","value":"__UNDEFINED__","cache":true,"required_inputs":["api_key"],"allows_loop":false,"tool_mode":true}],"field_order":["input_value","system_message","stream","max_output_tokens","model_name","api_key","top_p","temperature","n","top_k","tool_model_enabled"],"beta":false,"legacy":false,"edited":false,"metadata":{},"tool_mode":false,"lf_version":"1.2.0"},"showNode":true,"type":"GoogleGenerativeAIModel","id":"GoogleGenerativeAIModel-imboU"},"selected":true,"measured":{"width":320,"height":778}},{"id":"Prompt-upBoA","type":"genericNode","position":{"x":817.1554984489476,"y":530.5291342511027},"data":{"node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom import Component\nfrom langflow.inputs.inputs import DefaultPromptField\nfrom langflow.io import MessageTextInput, Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n        MessageTextInput(\n            name=\"tool_placeholder\",\n            display_name=\"Tool Placeholder\",\n            tool_mode=True,\n            advanced=True,\n            info=\"A placeholder input for tool mode.\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(self) -> Message:\n        prompt = Message.from_template(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    async def update_frontend_node(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"This function is called after the code validation is done.\"\"\"\n        frontend_node = await super().update_frontend_node(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"template":{"tool_mode":false,"trace_as_input":true,"list":false,"list_add_label":"Add More","required":false,"placeholder":"","show":true,"name":"template","value":"You have access to the message history here: {history}\n\n.........................................................................................................................................\nYour task:\nYou are a helpful AI chatbot that expert in recycling. You will help user by suggesting recycling guidelines in a patient and detailed manner. Your name is KitarBot. When user greet you, greet them back with in this format {greet}.\n\nRestriction:\nYou must only answer questions relating to recycle.  \nWhen user asked question after greet, do not greet back but straight answer the questions.\nWhen the user provide recycle item, straight suggest the colours of the recycle bin based on general interpretation which are blue, orange and chocolate. \n\nUser: {user}\nAi:","display_name":"Template","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"prompt","_input_type":"PromptInput"},"tool_placeholder":{"tool_mode":true,"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"list_add_label":"Add More","required":false,"placeholder":"","show":true,"name":"tool_placeholder","value":"","display_name":"Tool Placeholder","advanced":true,"input_types":["Message"],"dynamic":false,"info":"A placeholder input for tool mode.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"user":{"field_type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":true,"value":"","fileTypes":[],"file_path":"","name":"user","display_name":"user","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","load_from_db":false,"title_case":false,"type":"str"},"history":{"field_type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":true,"value":"","fileTypes":[],"file_path":"","name":"history","display_name":"history","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","load_from_db":false,"title_case":false,"type":"str"},"greet":{"field_type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":true,"value":"","fileTypes":[],"file_path":"","name":"greet","display_name":"greet","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","load_from_db":false,"title_case":false,"type":"str"}},"description":"Create a prompt template with dynamic variables.","icon":"prompts","is_input":null,"is_output":null,"is_composition":null,"base_classes":["Message"],"name":"","display_name":"Prompt","documentation":"","minimized":false,"custom_fields":{"template":["history","greet","user"]},"output_types":[],"full_path":null,"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"prompt","hidden":null,"display_name":"Prompt Message","method":"build_prompt","value":"__UNDEFINED__","cache":true,"required_inputs":null,"allows_loop":false,"tool_mode":true}],"field_order":["template","tool_placeholder"],"beta":false,"legacy":false,"error":null,"edited":false,"metadata":{},"tool_mode":false,"lf_version":"1.2.0"},"showNode":true,"type":"Prompt","id":"Prompt-upBoA"},"selected":false,"measured":{"width":320,"height":501}},{"id":"Memory-1YMJe","type":"genericNode","position":{"x":331.07478634713675,"y":425.9133544098696},"data":{"node":{"template":{"_type":"Component","memory":{"trace_as_metadata":true,"list":false,"list_add_label":"Add More","required":false,"placeholder":"","show":true,"name":"memory","value":"","display_name":"External Memory","advanced":false,"input_types":["Memory"],"dynamic":false,"info":"Retrieve messages from an external memory. If empty, it will use the Langflow tables.","title_case":false,"type":"other","_input_type":"HandleInput"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.custom import Component\nfrom langflow.helpers.data import data_to_text\nfrom langflow.inputs import HandleInput\nfrom langflow.io import DropdownInput, IntInput, MessageTextInput, MultilineInput, Output\nfrom langflow.memory import aget_messages\nfrom langflow.schema import Data\nfrom langflow.schema.message import Message\nfrom langflow.utils.constants import MESSAGE_SENDER_AI, MESSAGE_SENDER_USER\n\n\nclass MemoryComponent(Component):\n    display_name = \"Message History\"\n    description = \"Retrieves stored chat messages from Langflow tables or an external memory.\"\n    icon = \"message-square-more\"\n    name = \"Memory\"\n\n    inputs = [\n        HandleInput(\n            name=\"memory\",\n            display_name=\"External Memory\",\n            input_types=[\"Memory\"],\n            info=\"Retrieve messages from an external memory. If empty, it will use the Langflow tables.\",\n        ),\n        DropdownInput(\n            name=\"sender\",\n            display_name=\"Sender Type\",\n            options=[MESSAGE_SENDER_AI, MESSAGE_SENDER_USER, \"Machine and User\"],\n            value=\"Machine and User\",\n            info=\"Filter by sender type.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"sender_name\",\n            display_name=\"Sender Name\",\n            info=\"Filter by sender name.\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"n_messages\",\n            display_name=\"Number of Messages\",\n            value=100,\n            info=\"Number of messages to retrieve.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"session_id\",\n            display_name=\"Session ID\",\n            info=\"The session ID of the chat. If empty, the current session ID parameter will be used.\",\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"order\",\n            display_name=\"Order\",\n            options=[\"Ascending\", \"Descending\"],\n            value=\"Ascending\",\n            info=\"Order of the messages.\",\n            advanced=True,\n            tool_mode=True,\n        ),\n        MultilineInput(\n            name=\"template\",\n            display_name=\"Template\",\n            info=\"The template to use for formatting the data. \"\n            \"It can contain the keys {text}, {sender} or any other key in the message data.\",\n            value=\"{sender_name}: {text}\",\n            advanced=True,\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Data\", name=\"messages\", method=\"retrieve_messages\"),\n        Output(display_name=\"Message\", name=\"messages_text\", method=\"retrieve_messages_as_text\"),\n    ]\n\n    async def retrieve_messages(self) -> Data:\n        sender = self.sender\n        sender_name = self.sender_name\n        session_id = self.session_id\n        n_messages = self.n_messages\n        order = \"DESC\" if self.order == \"Descending\" else \"ASC\"\n\n        if sender == \"Machine and User\":\n            sender = None\n\n        if self.memory:\n            # override session_id\n            self.memory.session_id = session_id\n\n            stored = await self.memory.aget_messages()\n            # langchain memories are supposed to return messages in ascending order\n            if order == \"DESC\":\n                stored = stored[::-1]\n            if n_messages:\n                stored = stored[:n_messages]\n            stored = [Message.from_lc_message(m) for m in stored]\n            if sender:\n                expected_type = MESSAGE_SENDER_AI if sender == MESSAGE_SENDER_AI else MESSAGE_SENDER_USER\n                stored = [m for m in stored if m.type == expected_type]\n        else:\n            stored = await aget_messages(\n                sender=sender,\n                sender_name=sender_name,\n                session_id=session_id,\n                limit=n_messages,\n                order=order,\n            )\n        self.status = stored\n        return stored\n\n    async def retrieve_messages_as_text(self) -> Message:\n        stored_text = data_to_text(self.template, await self.retrieve_messages())\n        self.status = stored_text\n        return Message(text=stored_text)\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"n_messages":{"tool_mode":false,"trace_as_metadata":true,"list":false,"list_add_label":"Add More","required":false,"placeholder":"","show":true,"name":"n_messages","value":100,"display_name":"Number of Messages","advanced":true,"dynamic":false,"info":"Number of messages to retrieve.","title_case":false,"type":"int","_input_type":"IntInput"},"order":{"tool_mode":true,"trace_as_metadata":true,"options":["Ascending","Descending"],"options_metadata":[],"combobox":false,"dialog_inputs":{},"required":false,"placeholder":"","show":true,"name":"order","value":"Ascending","display_name":"Order","advanced":true,"dynamic":false,"info":"Order of the messages.","title_case":false,"type":"str","_input_type":"DropdownInput"},"sender":{"tool_mode":false,"trace_as_metadata":true,"options":["Machine","User","Machine and User"],"options_metadata":[],"combobox":false,"dialog_inputs":{},"required":false,"placeholder":"","show":true,"name":"sender","value":"Machine and User","display_name":"Sender Type","advanced":true,"dynamic":false,"info":"Filter by sender type.","title_case":false,"type":"str","_input_type":"DropdownInput"},"sender_name":{"tool_mode":false,"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"list_add_label":"Add More","required":false,"placeholder":"","show":true,"name":"sender_name","value":"","display_name":"Sender Name","advanced":true,"input_types":["Message"],"dynamic":false,"info":"Filter by sender name.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"session_id":{"tool_mode":false,"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"list_add_label":"Add More","required":false,"placeholder":"","show":true,"name":"session_id","value":"","display_name":"Session ID","advanced":true,"input_types":["Message"],"dynamic":false,"info":"The session ID of the chat. If empty, the current session ID parameter will be used.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"template":{"tool_mode":false,"trace_as_input":true,"multiline":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"list_add_label":"Add More","required":false,"placeholder":"","show":true,"name":"template","value":"{sender_name}: {text}","display_name":"Template","advanced":true,"input_types":["Message"],"dynamic":false,"info":"The template to use for formatting the data. It can contain the keys {text}, {sender} or any other key in the message data.","title_case":false,"type":"str","_input_type":"MultilineInput"}},"description":"Retrieves stored chat messages from Langflow tables or an external memory.","icon":"message-square-more","base_classes":["Data","Message"],"display_name":"Message History","documentation":"","minimized":false,"custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Data"],"selected":"Data","name":"messages","display_name":"Data","method":"retrieve_messages","value":"__UNDEFINED__","cache":true,"allows_loop":false,"tool_mode":true},{"types":["Message"],"selected":"Message","name":"messages_text","display_name":"Message","method":"retrieve_messages_as_text","value":"__UNDEFINED__","cache":true,"allows_loop":false,"tool_mode":true}],"field_order":["memory","sender","sender_name","n_messages","session_id","order","template"],"beta":false,"legacy":false,"edited":false,"metadata":{},"tool_mode":false,"category":"helpers","key":"Memory","score":0.007568328950209746,"lf_version":"1.2.0"},"showNode":true,"type":"Memory","id":"Memory-1YMJe"},"selected":false,"measured":{"width":320,"height":259}},{"id":"TextInput-KlD03","type":"genericNode","position":{"x":300,"y":739.5844408645938},"data":{"node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.io.text import TextComponent\nfrom langflow.io import MultilineInput, Output\nfrom langflow.schema.message import Message\n\n\nclass TextInputComponent(TextComponent):\n    display_name = \"Text Input\"\n    description = \"Get text inputs from the Playground.\"\n    icon = \"type\"\n    name = \"TextInput\"\n\n    inputs = [\n        MultilineInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            info=\"Text to be passed as input.\",\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Message\", name=\"text\", method=\"text_response\"),\n    ]\n\n    def text_response(self) -> Message:\n        return Message(\n            text=self.input_value,\n        )\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"input_value":{"tool_mode":false,"trace_as_input":true,"multiline":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"list_add_label":"Add More","required":false,"placeholder":"","show":true,"name":"input_value","value":"Hi there! I'm RecycleBot, your friendly AI helper here to make recycling easy for you. How can I assist you today? 😊\n","display_name":"Text","advanced":false,"input_types":["Message"],"dynamic":false,"info":"Text to be passed as input.","title_case":false,"type":"str","_input_type":"MultilineInput"}},"description":"Get text inputs from the Playground.","icon":"type","base_classes":["Message"],"display_name":"Text Input","documentation":"","minimized":false,"custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"text","display_name":"Message","method":"text_response","value":"__UNDEFINED__","cache":true,"allows_loop":false,"tool_mode":true}],"field_order":["input_value"],"beta":false,"legacy":false,"edited":false,"metadata":{},"tool_mode":false,"lf_version":"1.2.0"},"showNode":true,"type":"TextInput","id":"TextInput-KlD03"},"selected":false,"measured":{"width":320,"height":229}}],"edges":[{"source":"GoogleGenerativeAIModel-imboU","target":"ChatOutput-vikhM","sourceHandle":"{œdataTypeœ:œGoogleGenerativeAIModelœ,œidœ:œGoogleGenerativeAIModel-imboUœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}","targetHandle":"{œfieldNameœ:œinput_valueœ,œidœ:œChatOutput-vikhMœ,œinputTypesœ:[œDataœ,œDataFrameœ,œMessageœ],œtypeœ:œotherœ}","id":"reactflow__edge-GoogleGenerativeAIModel-imboU{œdataTypeœ:œGoogleGenerativeAIModelœ,œidœ:œGoogleGenerativeAIModel-imboUœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}-ChatOutput-vikhM{œfieldNameœ:œinput_valueœ,œidœ:œChatOutput-vikhMœ,œinputTypesœ:[œDataœ,œDataFrameœ,œMessageœ],œtypeœ:œotherœ}","data":{"sourceHandle":{"dataType":"GoogleGenerativeAIModel","id":"GoogleGenerativeAIModel-imboU","name":"text_output","output_types":["Message"]},"targetHandle":{"fieldName":"input_value","id":"ChatOutput-vikhM","inputTypes":["Data","DataFrame","Message"],"type":"other"}},"selected":false,"className":"","animated":false},{"source":"Prompt-upBoA","target":"GoogleGenerativeAIModel-imboU","sourceHandle":"{œdataTypeœ:œPromptœ,œidœ:œPrompt-upBoAœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}","targetHandle":"{œfieldNameœ:œinput_valueœ,œidœ:œGoogleGenerativeAIModel-imboUœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","id":"reactflow__edge-Prompt-upBoA{œdataTypeœ:œPromptœ,œidœ:œPrompt-upBoAœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-GoogleGenerativeAIModel-imboU{œfieldNameœ:œinput_valueœ,œidœ:œGoogleGenerativeAIModel-imboUœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","data":{"sourceHandle":{"dataType":"Prompt","id":"Prompt-upBoA","name":"prompt","output_types":["Message"]},"targetHandle":{"fieldName":"input_value","id":"GoogleGenerativeAIModel-imboU","inputTypes":["Message"],"type":"str"}},"selected":false,"className":"","animated":false},{"source":"Memory-1YMJe","target":"Prompt-upBoA","sourceHandle":"{œdataTypeœ:œMemoryœ,œidœ:œMemory-1YMJeœ,œnameœ:œmessages_textœ,œoutput_typesœ:[œMessageœ]}","targetHandle":"{œfieldNameœ:œhistoryœ,œidœ:œPrompt-upBoAœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","id":"reactflow__edge-Memory-1YMJe{œdataTypeœ:œMemoryœ,œidœ:œMemory-1YMJeœ,œnameœ:œmessages_textœ,œoutput_typesœ:[œMessageœ]}-Prompt-upBoA{œfieldNameœ:œhistoryœ,œidœ:œPrompt-upBoAœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","data":{"sourceHandle":{"dataType":"Memory","id":"Memory-1YMJe","name":"messages_text","output_types":["Message"]},"targetHandle":{"fieldName":"history","id":"Prompt-upBoA","inputTypes":["Message"],"type":"str"}},"selected":false,"className":"","animated":false},{"source":"ChatInput-feB0F","target":"Prompt-upBoA","sourceHandle":"{œdataTypeœ:œChatInputœ,œidœ:œChatInput-feB0Fœ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}","targetHandle":"{œfieldNameœ:œuserœ,œidœ:œPrompt-upBoAœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","id":"reactflow__edge-ChatInput-feB0F{œdataTypeœ:œChatInputœ,œidœ:œChatInput-feB0Fœ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}-Prompt-upBoA{œfieldNameœ:œuserœ,œidœ:œPrompt-upBoAœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","data":{"sourceHandle":{"dataType":"ChatInput","id":"ChatInput-feB0F","name":"message","output_types":["Message"]},"targetHandle":{"fieldName":"user","id":"Prompt-upBoA","inputTypes":["Message"],"type":"str"}},"selected":false,"className":"","animated":false},{"source":"TextInput-KlD03","target":"Prompt-upBoA","sourceHandle":"{œdataTypeœ:œTextInputœ,œidœ:œTextInput-KlD03œ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}","targetHandle":"{œfieldNameœ:œgreetœ,œidœ:œPrompt-upBoAœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","id":"reactflow__edge-TextInput-KlD03{œdataTypeœ:œTextInputœ,œidœ:œTextInput-KlD03œ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-Prompt-upBoA{œfieldNameœ:œgreetœ,œidœ:œPrompt-upBoAœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","data":{"sourceHandle":{"dataType":"TextInput","id":"TextInput-KlD03","name":"text","output_types":["Message"]},"targetHandle":{"fieldName":"greet","id":"Prompt-upBoA","inputTypes":["Message"],"type":"str"}},"selected":false,"className":"","animated":false}],"viewport":{"x":-141.05969038240016,"y":-217.24131622409425,"zoom":0.6847444224867889}},"is_component":false}